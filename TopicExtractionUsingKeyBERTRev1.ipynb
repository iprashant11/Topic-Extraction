{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02ca401-6103-45b7-b4d1-a2fffc88ca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prashanti\\.conda\\envs\\rag-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from keybert import KeyBERT\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.data import find\n",
    "import nltk\n",
    "import re\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Download required NLTK resources if not already available\n",
    "try:\n",
    "    find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Topic Extraction API\",\n",
    "    description=\"Extracts topics from raw text using KeyBERT + sentence chunking + semantic deduplication\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Load models globally\n",
    "kw_model = KeyBERT()\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Request body model\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "    n_sentences: int = 5\n",
    "    top_n: int = 3\n",
    "    max_topics: int = 10  # Max number of topics after deduplication\n",
    "\n",
    "# Response model\n",
    "class TopicResponse(BaseModel):\n",
    "    topics: List[str]\n",
    "\n",
    "# Utility to clean input text\n",
    "def clean_text(text: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Chunk text and extract keywords with scores\n",
    "def extract_topics_by_chunk(text: str, n_sentences: int = 5, top_n: int = 3) -> List[tuple]:\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = [' '.join(sentences[i:i + n_sentences]) for i in range(0, len(sentences), n_sentences)]\n",
    "    \n",
    "    all_keywords = []\n",
    "    for chunk in chunks:\n",
    "        keywords = kw_model.extract_keywords(chunk, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=top_n)\n",
    "        all_keywords.extend(keywords)  # List of (keyword, score)\n",
    "\n",
    "    return all_keywords\n",
    "\n",
    "# Semantic deduplication (keep highest scored topic among similar ones)\n",
    "def semantically_deduplicate_scored(topics_scored: List[tuple], similarity_threshold: float = 0.75) -> List[tuple]:\n",
    "    if len(topics_scored) < 2:\n",
    "        return topics_scored\n",
    "\n",
    "    keywords = [kw for kw, _ in topics_scored]\n",
    "    embeddings = semantic_model.encode(keywords, convert_to_tensor=True)\n",
    "\n",
    "    retained = []\n",
    "    used = set()\n",
    "\n",
    "    for i in range(len(keywords)):\n",
    "        if i in used:\n",
    "            continue\n",
    "        retained.append((keywords[i], topics_scored[i][1]))  # (keyword, score)\n",
    "        for j in range(i + 1, len(keywords)):\n",
    "            if j in used:\n",
    "                continue\n",
    "            score = util.cos_sim(embeddings[i], embeddings[j])\n",
    "            if score.item() > similarity_threshold:\n",
    "                used.add(j)\n",
    "\n",
    "    return retained\n",
    "\n",
    "# API endpoint\n",
    "@app.post(\"/extract-topics\", response_model=TopicResponse)\n",
    "def extract_topics(request: TextRequest):\n",
    "    if not request.text.strip():\n",
    "        raise HTTPException(status_code=400, detail=\"Text input cannot be empty.\")\n",
    "\n",
    "    cleaned_text = clean_text(request.text)\n",
    "    topics_with_scores = extract_topics_by_chunk(cleaned_text, request.n_sentences, request.top_n)\n",
    "    deduped = semantically_deduplicate_scored(topics_with_scores)\n",
    "\n",
    "    # Sort by importance (score descending), limit to max_topics\n",
    "    deduped_sorted = sorted(deduped, key=lambda x: x[1], reverse=True)\n",
    "    final_keywords = [kw for kw, _ in deduped_sorted[:request.max_topics]]\n",
    "\n",
    "    return {\"topics\": final_keywords}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815ab91-b015-4709-af77-66cd6a309f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag-env)",
   "language": "python",
   "name": "rag-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
